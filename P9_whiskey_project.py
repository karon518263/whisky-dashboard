import requests
from bs4 import BeautifulSoup
import time
import random
import re
import sqlite3
from datetime import datetime, timedelta
import pandas as pd
import os

DRY_RUN = False #True = è©¦è·‘(ä¸å­˜)ï¼Œ False = æ­£å¼(å­˜æª”)
DB_NAME = 'P9_whiskey_project.db'

#æŠ“å–çš„å¤©æ•¸
DAYS_TO_SCRAPE = 7

TARGET_DATE = (datetime.now() - timedelta(days=DAYS_TO_SCRAPE)).strftime('%Y/%m/%d')

MISS_LIMIT = 40 
MAX_PAGES = 30


cutoff_date = datetime.now() - timedelta(days=DAYS_TO_SCRAPE)
cutoff_str = cutoff_date.strftime('%Y/%m/%d')


#è³‡æ–™åº«
def init_db():
    with sqlite3.connect(DB_NAME) as conn:
        conn.execute('''
            CREATE TABLE IF NOT EXISTS market_prices (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                post_date TEXT,
                title TEXT, 
                author TEXT, 
                product_name TEXT,
                price INTEGER, 
                link TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')

# å­˜æª”
def save_to_db (item):
    with sqlite3.connect(DB_NAME) as conn:
        cursor = conn.cursor()
        check = cursor.execute(
            'SELECT id FROM market_prices WHERE post_date=? AND author=? AND product_name=? AND price=?',
            (item['date'], item['author'], item['name'], item['price'])
        ).fetchone()

        if not check:
            cursor.execute(
                'INSERT INTO market_prices (post_date, title, author, product_name, price, link) VALUES (?, ?, ?, ?, ?, ?)',
                (item['date'], item['title'], item['author'], item['name'], item['price'], item['link'])
            )
            return True
    return False


def clean_string(text):
    """
    åªä¿ç•™ï¼šä¸­æ–‡ã€è‹±æ–‡(a-z, A-Z)ã€æ•¸å­—(0-9)
    å»é™¤ï¼šæ‰€æœ‰æ¨™é»ç¬¦è™Ÿã€æ‹¬è™Ÿã€ç‰¹æ®Šå­—å…ƒã€Emojiã€ç©ºæ ¼(è¦–éœ€æ±‚è€Œå®š)
    """
    if not text: return ""
    # Regex é‚è¼¯ï¼š [^\u4e00-\u9fa5a-zA-Z0-9] ä»£è¡¨ã€Œé™¤äº†ä¸­è‹±æ•¸ä»¥å¤–çš„æ‰€æœ‰å­—å…ƒã€
    # å¦‚æœä½ æƒ³ã€Œä¿ç•™ç©ºæ ¼ã€ï¼Œè«‹å°‡ regex æ”¹ç‚º r'[^\u4e00-\u9fa5a-zA-Z0-9\s]'
    text = text.replace('Ã—', 'x').replace('ï¼Š', '*').replace('X', 'x')
    cleaned = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s/.\-*]', '', text)
    return cleaned.strip()

# æŠ“å–å·¥å…·å‡½æ•¸

def get_title(soup):
    try:
        title_node = soup.find('span', id = re.compile(r'lblSubject'))
        if title_node:
            return title_node.text.strip()
    except: pass
    return None


def get_date(soup):
    try:
        date_meta = soup.find('meta', property='og:image')
        if date_meta and date_meta.get('content'):
            dm = re.search(r'/(\d{4}/\d{2}/\d{2})/', date_meta['content'])
            if dm: return dm.group(1)
    except: pass
    try:
        text = soup.get_text(separator=' ', strip=True)
        full_time_match = re.search(r'(\d{4}/\d{2}/\d{2})\s+\d{2}:\d{2}:\d{2}', text)
        if full_time_match: return full_time_match.group(1)
    except: pass
    try:
        fallback_match = re.search(r'ç™¼è¡¨æ™‚é–“\D*(\d{4}/\d{2}/\d{2})', text)
        if fallback_match: return fallback_match.group(1)
    except: pass
    return 'æœªçŸ¥'

def get_author(soup):
    try:
        author_td = soup.find('td', class_='pricelist_01')
        if author_td:
            parts = author_td.get_text(separator='\n', strip=True).split('\n')
            if parts:
                name = parts[0].strip()
                if "è©•åƒ¹" not in name and len(name) > 0: return name
    except: pass
    return "æœªçŸ¥"

def main(DRY_RUN=True):
    if not DRY_RUN : init_db()

    base_url = "https://www.p9.com.tw"
    base_list_url = 'https://www.p9.com.tw/Forum/ForumSection.aspx?id=1&BoardId=5&Sort=Post_Time'
    headers = { "User-Agent": "Mozilla/5.0" }

    session = requests.Session()
    session.headers.update(headers)

    today_date = datetime.now()
    page = 1
    total_added = 0
    stop_crawling = False
    consecutive_miss_count = 0

    blacklist = ['å…¨éƒ¨','ç§','ä¸€èµ·','ç¸½å…±','å…±','å¸¶é‡Œç¨‹','èµ·è·‘','å¾µ']
    spec_keywords =['cask','no','no:','æ¡¶è™Ÿ']
    seen_items = set()

    while page <= MAX_PAGES and not stop_crawling:
        print(f'[é€²åº¦]è®€å–ç¬¬{page}é ...(ç›®å‰ç´¯ç©è½ç©º:{ consecutive_miss_count}ç¯‡)')
        try:
            resp = session.get(f'{base_list_url}&Page={page}')
            soup = BeautifulSoup(resp.content, 'html.parser')
            posts = soup.find_all('a', class_='a18')

            if not posts: break

            target_posts = posts

            for post in target_posts:
                list_title = post.text.strip()
                link = base_url + post.get('href')

                #æ¨™é¡Œæœ‰é€™äº›é—œéµå­—å°±è·³é
                if any(k in list_title for k in ['å¾µ','æ”¶','æ¯','è·¯è·‘','æ®˜åŠ‘']): continue

                try:
                    resp_inner = session.get(link)
                    soup_inner = BeautifulSoup(resp_inner.text, 'html.parser')

                    post_date = get_date(soup_inner)

                    #æŠ“å–å®Œæ•´æ¨™é¡Œ
                    inner_title = get_title(soup_inner)
                    final_title = inner_title if inner_title else list_title

                    #æ—¥æœŸåˆ¤æ–·
                    if post_date != 'æœªçŸ¥':
                        if post_date <= cutoff_str:
                            consecutive_miss_count += 1
                            if consecutive_miss_count >= MISS_LIMIT:
                                print(f'é€£çºŒ{MISS_LIMIT} ç¯‡æ—©æ–¼ {cutoff_str}ï¼Œä»»å‹™çµæŸ!')
                                stop_crawling = True
                                break
                            time.sleep(random.uniform(1, 2))
                            continue
                        elif post_date > today_date.strftime('%Y/%m/%d'):
                            continue
                        else:
                            consecutive_miss_count = 0

                    #æŠ“å–å…§å®¹
                    author = get_author(soup_inner)
                    content_tag = soup_inner.find('td' , class_='editZone')
                    if content_tag:
                        raw_text = content_tag.get_text(separator='\n', strip=True)
                        lines = raw_text.split('\n')

                        for line in lines:
                            line = line.strip()
                            if len(line) < 3 : continue
                            #ç§»é™¤åƒåˆ†è™Ÿ
                            clean = line.replace(',','').replace('ï¼Œ','')

                            clean_line = re.sub(r'\d+\s*(?:ml|mL|ML|cc|CC|cl|CL|l|L)(?![a-zA-Z])', '', clean, flags=re.IGNORECASE)

                            regex_pattern = r'(.*)(?:\s+|ï¼|=|:)(\d+)(\s*(?:è·‘|å…¬å°º|k|K|å«.*|/ç“¶|/ä¸€ç“¶|/çµ„|/ä¸€çµ„|/B|/b|/JPEG|/JPG|åƒç´ |é‡Œç¨‹|å…é‹|è²“|å¤§å˜´é³¥|/ä½|.*)?)?$'
                            match = re.search(regex_pattern, clean_line)

                            match_fallback = None
                            if not match :
                                match_fallback = re.search(r'(?:è·‘|å…¬å°º|é‡Œ|JPEG|NT|\$)\s*(\d+)', clean, re.IGNORECASE)

                            raw_name, final_price = '', 0
                            if match:
                                raw_name = match.group(1).strip()
                                final_price = int(match.group(2))
                                
                                # ã€æ–°å¢ã€‘æª¢æŸ¥å¾Œç¶´çš„æ•¸é‡æ¨™ç¤º
                                suffix = match.group(3)
                                if suffix:
                                    # æƒ…æ³ 1: æ˜ç¢ºæ•¸å­— (å¦‚ /2, /3b, /2ç“¶) -> æ¨™è¨˜ç‚º /N
                                    qty_match = re.search(r'/(\d+)', suffix)
                                    if qty_match:
                                        qty = qty_match.group(1)
                                        raw_name += f" /{qty}"
                                    # æƒ…æ³ 2: åƒ…æœ‰å–®ä½ç„¡æ•¸å­— (å¦‚ /ä½, /ç“¶) -> æ¨™è¨˜ç‚º /1 (å–®åƒ¹)
                                    elif re.search(r'/[ä½ç“¶bBæ”¯ç½çµ„]', suffix):
                                        raw_name += " /1"
                            elif match_fallback:
                                raw_name = clean_line
                                final_price = int(match_fallback.group(1))


                            if final_price > 0:
                                if final_price > 500000 or final_price < 800: continue
                                if 1950 < final_price < 2030:
                                    if not any(s in line for s in ['è·‘','å«','å…é‹','JPEG','JPG','K','k','/']):continue

                                final_name = clean_string(raw_name)
                                clean_title = clean_string(final_title)

                                check_str = (final_name + clean_title).lower()



                                if any(bad in check_str for bad in blacklist): continue
                                if any(x in check_str for x in spec_keywords): continue
                                if author != 'æœªçŸ¥' and author in final_name: continue
                                if (final_name, final_price) in seen_items: continue
                                seen_items.add((final_name, final_price))



                                item = {
                                    'date': post_date,
                                    'title': final_title,
                                    'author': author,
                                    'name': final_name[:35],
                                    'price': final_price,
                                    'link':link
                                }

                                log_msg = f"[{item['date']}] {item['author']} - {item['name'][:10]} ${item['price']}"

                                if DRY_RUN:
                                    print(f"[æ¨¡æ“¬] {log_msg} (æ¨™é¡Œ: {item['title'][:10]}...)")

                                else:
                                    if save_to_db(item):
                                        print(f' [å­˜æª”] {log_msg}')
                                        total_added += 1
                    
                    sleep_time = random.uniform(1.5, 3.5)
                    time.sleep(sleep_time)

                except Exception as e:
                    print(f"  [éŒ¯èª¤] è™•ç†å…§é å¤±æ•—: {e}")
                    time.sleep(2)
                    continue


            page += 1


        except Exception as e:
            print(f"[éŒ¯èª¤] è®€å–åˆ—è¡¨é å¤±æ•—: {e}")
            break

    print(f"\n--- çˆ¬èŸ²çµæŸï¼Œå…±æ–°å¢ {total_added} ç­†è³‡æ–™ ---")


        # é©—è­‰
    if not DRY_RUN and total_added >= 0:
        print("\nğŸ“Š é©—è­‰æœ€æ–°è³‡æ–™ï¼š")
        try:
            conn = sqlite3.connect(DB_NAME)
            df = pd.read_sql_query("SELECT post_date, title, price FROM market_prices ORDER BY id DESC LIMIT 5", conn)
            print(df)
            conn.close()
        except: pass


if __name__ == '__main__':
    main(DRY_RUN=DRY_RUN)


            